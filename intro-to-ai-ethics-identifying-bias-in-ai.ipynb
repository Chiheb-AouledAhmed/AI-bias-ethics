{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014195,
     "end_time": "2020-10-30T19:06:15.040067",
     "exception": false,
     "start_time": "2020-10-30T19:06:15.025872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the tutorial, you learned about six different types of bias.  In this exercise, you'll train a model with **real data** and get practice with identifying bias.  Don't worry if you're new to coding: you'll still be able to complete the exercise!\n",
    "\n",
    "# Introduction\n",
    "\n",
    "At the end of 2017, the [Civil Comments](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) platform shut down and released their ~2 million public comments in a lasting open archive. Jigsaw sponsored this effort and helped to comprehensively annotate the data.  In 2019, Kaggle held the [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview) competition so that data scientists worldwide could work together to investigate ways to mitigate bias.\n",
    "\n",
    "The code cell below loads some of the data from the competition.  We'll work with thousands of comments, where each comment is labeled as either \"toxic\" or \"not toxic\".\n",
    "\n",
    "Begin by running the next code cell.  \n",
    "- Clicking inside the code cell.\n",
    "- Click on the triangle (in the shape of a \"Play button\") that appears to the left of the code cell.\n",
    "\n",
    "The code will run for approximately 30 seconds.  When it finishes, you should see as output a message saying that the data was successfully loaded, along with two examples of comments: one is toxic, and the other is not.\n",
    "\n",
    "> **Optional** note: The original competition data uses a toxicity score ranging from 0 to 1.  We've simplified this score to either 0 or 1 by thresholding the value: scores > 0.7 are assigned \"1\", scores < 0.3 are assigned \"0\", and comments with scores between 0.3 and 0.7 are dropped from the dataset.  Additionally, to reduce runtime, we have reduced the size of the dataset with subsampling. To preprocess the comments, we use a \"bag of words\" approach with `CountVectorizer()`.  Note that this is a simple approach, and in practice you'll want to spend time cleaning up the data.  Here's a great example for how to do that: https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 30.692706,
     "end_time": "2020-10-30T19:06:45.747191",
     "exception": false,
     "start_time": "2020-10-30T19:06:15.054485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded!\n",
      "\n",
      "Sample toxic comment: Yeah, just like you, you moron.\n",
      "Sample not-toxic comment: I'm lovin' it.\n"
     ]
    }
   ],
   "source": [
    "# Set up feedback system\n",
    "\"\"\"from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.ethics.ex3 import *\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Get the same results each time\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the (full) training data\n",
    "full_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Work with a small subset of the data: if target > 0.7, toxic.  If target < 0.3, non-toxic\n",
    "full_toxic = full_data[full_data[\"target\"]>0.7]\n",
    "full_nontoxic = full_data[full_data[\"target\"]<0.3].sample(len(full_toxic))\n",
    "data = pd.concat([full_toxic, full_nontoxic], ignore_index=True)\n",
    "comments = data[\"comment_text\"]\n",
    "target = (data[\"target\"]>0.7).astype(int)\n",
    "\n",
    "# Break into training and test sets\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n",
    "\n",
    "# Get vocabulary from training data\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(comments_train)\n",
    "\n",
    "# Get word counts for training and test sets\n",
    "X_train = vectorizer.transform(comments_train)\n",
    "X_test = vectorizer.transform(comments_test)\n",
    "\n",
    "# Preview the dataset\n",
    "print(\"Data successfully loaded!\\n\")\n",
    "print(\"Sample toxic comment:\", comments_train.iloc[18])\n",
    "print(\"Sample not-toxic comment:\", comments_train.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45) (45451, 45) (45451, 45) (90902, 45) (63631,) (27271,)\n"
     ]
    }
   ],
   "source": [
    "print(full_data.shape, full_toxic.shape, full_nontoxic.shape, data.shape, comments_train.shape, comments_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43081    As the title of the editorial says “the reacti...\n",
       "10787    They're all guilty, I wish we had the death pe...\n",
       "65788    The USA has exactly the opposite problem that ...\n",
       "66265                                       I'm lovin' it.\n",
       "72267    I agree. We will never tax ourselves into pros...\n",
       "13200    I wouldn't have known you are black unless you...\n",
       "44885    A coup .... about bloody time!\\n\\nRelatives sh...\n",
       "4514     I say we take the little bitch and his loyalis...\n",
       "86531    We CANNOT \"borrow our way to prosperity\" any m...\n",
       "41724            He is a delusional psychopath. Neohitler.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(comments_train))\n",
    "comments_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (63631, 58041)\n",
      "58041\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), X_train.shape)\n",
    "print(len(X_train[0].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X_train[0,:].toarray()[0]))\n",
    "X_train[0,:].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X_train[:,0].toarray()))\n",
    "X_train[:,0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012727,
     "end_time": "2020-10-30T19:06:45.774334",
     "exception": false,
     "start_time": "2020-10-30T19:06:45.761607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Run the next code cell without changes to use the data to train a simple model.  The output shows the accuracy of the model on some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 11.047454,
     "end_time": "2020-10-30T19:06:56.836704",
     "exception": false,
     "start_time": "2020-10-30T19:06:45.78925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chihe\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9294488650947893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a model and evaluate performance on test dataset\n",
    "classifier = LogisticRegression(max_iter=2000)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "# Function to classify any string\n",
    "def classify_string(string, investigate=False):\n",
    "    prediction = classifier.predict(vectorizer.transform([string]))[0]\n",
    "    if prediction == 0:\n",
    "        print(\"NOT TOXIC:\", string)\n",
    "    else:\n",
    "        print(\"TOXIC:\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014849,
     "end_time": "2020-10-30T19:06:56.868884",
     "exception": false,
     "start_time": "2020-10-30T19:06:56.854035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Roughly 93% of the comments in the test data are classified correctly!\n",
    "\n",
    "# 1) Try out the model\n",
    "\n",
    "You'll use the next code cell to write your own comments and supply them to the model: does the model classify them as toxic?  \n",
    "\n",
    "1. Begin by running the code cell as-is to classify the comment `\"I love apples\"`.  You should see that was classified as \"NOT TOXIC\".\n",
    "\n",
    "2. Then, try out another comment: `\"Apples are stupid\"`.  To do this, change only `\"I love apples\"` and leaving the rest of the code as-is.  Make sure that your comment is enclosed in quotes, as below.\n",
    "```python\n",
    "my_comment = \"Apples are stupid\"\n",
    "```\n",
    "3. Try out several comments (not necessarily about apples!), to see how the model performs: does it perform as suspected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.026189,
     "end_time": "2020-10-30T19:06:56.90901",
     "exception": false,
     "start_time": "2020-10-30T19:06:56.882821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT TOXIC: I love apples\n"
     ]
    }
   ],
   "source": [
    "# Comment to pass through the model\n",
    "my_comment = \"I love apples\"\n",
    "#my_comment = comments_train.iloc[np.random.randint(0,100,1)[0]]\n",
    "\n",
    "# Do not change the code below\n",
    "classify_string(my_comment)\n",
    "#q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018539,
     "end_time": "2020-10-30T19:06:56.942426",
     "exception": false,
     "start_time": "2020-10-30T19:06:56.923887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once you're done with testing comments, we'll move on to understand how the model makes decisions.  Run the next code cell without changes.\n",
    "\n",
    "The model assigns each of roughly 58,000 words a coefficient, where higher coefficients denote words that the model thinks are more toxic.  The code cell outputs the ten words that are considered most toxic, along with their coefficients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00</td>\n",
       "      <td>0.431559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000</td>\n",
       "      <td>-0.155165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000000000000000000</td>\n",
       "      <td>-0.025017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001</td>\n",
       "      <td>-0.173488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001</td>\n",
       "      <td>-0.044941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58036</th>\n",
       "      <td>𝒑𝒖𝒃𝒍𝒊𝒄𝒍𝒚</td>\n",
       "      <td>0.000301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58037</th>\n",
       "      <td>𝒑𝒖𝒓𝒄𝒉𝒂𝒔𝒆</td>\n",
       "      <td>0.000301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58038</th>\n",
       "      <td>𝒕𝒉𝒆</td>\n",
       "      <td>0.000301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58039</th>\n",
       "      <td>𝒕𝒐</td>\n",
       "      <td>0.000301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58040</th>\n",
       "      <td>𝓒𝓲𝓿𝓲𝓵</td>\n",
       "      <td>-0.109584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58041 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word     coeff\n",
       "0                       00  0.431559\n",
       "1                      000 -0.155165\n",
       "2      0000000000000000000 -0.025017\n",
       "3                    00001 -0.173488\n",
       "4                     0001 -0.044941\n",
       "...                    ...       ...\n",
       "58036             𝒑𝒖𝒃𝒍𝒊𝒄𝒍𝒚  0.000301\n",
       "58037             𝒑𝒖𝒓𝒄𝒉𝒂𝒔𝒆  0.000301\n",
       "58038                  𝒕𝒉𝒆  0.000301\n",
       "58039                   𝒕𝒐  0.000301\n",
       "58040                𝓒𝓲𝓿𝓲𝓵 -0.109584\n",
       "\n",
       "[58041 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.115908,
     "end_time": "2020-10-30T19:06:57.07637",
     "exception": false,
     "start_time": "2020-10-30T19:06:56.960462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25848</th>\n",
       "      <td>hypocrite</td>\n",
       "      <td>6.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16985</th>\n",
       "      <td>dumb</td>\n",
       "      <td>6.446921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12995</th>\n",
       "      <td>crap</td>\n",
       "      <td>6.519769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34285</th>\n",
       "      <td>moron</td>\n",
       "      <td>6.626779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38378</th>\n",
       "      <td>pathetic</td>\n",
       "      <td>6.643814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26015</th>\n",
       "      <td>idiotic</td>\n",
       "      <td>6.669001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49888</th>\n",
       "      <td>stupidity</td>\n",
       "      <td>7.503452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26021</th>\n",
       "      <td>idiots</td>\n",
       "      <td>8.549985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26013</th>\n",
       "      <td>idiot</td>\n",
       "      <td>8.637935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49876</th>\n",
       "      <td>stupid</td>\n",
       "      <td>9.369515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     coeff\n",
       "25848  hypocrite  6.218600\n",
       "16985       dumb  6.446921\n",
       "12995       crap  6.519769\n",
       "34285      moron  6.626779\n",
       "38378   pathetic  6.643814\n",
       "26015    idiotic  6.669001\n",
       "49888  stupidity  7.503452\n",
       "26021     idiots  8.549985\n",
       "26013      idiot  8.637935\n",
       "49876     stupid  9.369515"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients.sort_values(by=['coeff']).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015335,
     "end_time": "2020-10-30T19:06:57.106908",
     "exception": false,
     "start_time": "2020-10-30T19:06:57.091573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2) Most toxic words\n",
    "\n",
    "Take a look at the most toxic words from the code cell above.  Are you surprised to see any of them?  Are there any words that seem like they should not be in the list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.022976,
     "end_time": "2020-10-30T19:06:57.145202",
     "exception": false,
     "start_time": "2020-10-30T19:06:57.122226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to get credit!)\n",
    "#q_2.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015333,
     "end_time": "2020-10-30T19:06:57.175887",
     "exception": false,
     "start_time": "2020-10-30T19:06:57.160554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3) A closer investigation\n",
    "\n",
    "We'll take a closer look at how the model classifies comments.\n",
    "1. Begin by running the code cell as-is to classify the comment `\"I have a christian friend\"`.  You should see that was classified as \"NOT TOXIC\".  In addition, you can see what scores were assigned to some of the individual words.  Note that all words in the comment likely won't appear.\n",
    "2. Next, try out another comment: `\"I have a muslim friend\"`.  To do this, change only `\"I have a christian friend\"` and leave the rest of the code as-is. Make sure that your comment is enclosed in quotes, as below.\n",
    "```python\n",
    "new_comment = \"I have a muslim friend\"\n",
    "```\n",
    "3. Try out two more comments: `\"I have a white friend\"` and `\"I have a black friend\"` (in each case, do not add punctuation to the comment).\n",
    "4. Feel free to try out more comments, to see how the model classifies them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 0.044861,
     "end_time": "2020-10-30T19:06:57.236559",
     "exception": false,
     "start_time": "2020-10-30T19:06:57.191698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT TOXIC: I have a good friend\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21422</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.055318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22755</th>\n",
       "      <td>good</td>\n",
       "      <td>-0.277616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24208</th>\n",
       "      <td>have</td>\n",
       "      <td>-0.066489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word     coeff\n",
       "21422  friend  0.055318\n",
       "22755    good -0.277616\n",
       "24208    have -0.066489"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the value of new_comment\n",
    "new_comment = \"I have a christian friend\"\n",
    "new_comment = \"I have a good friend\"\n",
    "\n",
    "# Do not change the code below\n",
    "classify_string(new_comment)\n",
    "coefficients[coefficients.word.isin(new_comment.split())]\n",
    "#q_3.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016152,
     "end_time": "2020-10-30T19:06:57.26916",
     "exception": false,
     "start_time": "2020-10-30T19:06:57.253008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4) Identify bias\n",
    "\n",
    "Do you see any signs of potential bias in the model?  In the code cell above,\n",
    "- How did the model classify `\"I have a christian friend\"` and `\"I have a muslim friend\"`?  \n",
    "- How did it classify `\"I have a white friend\"` and `\"I have a black friend\"`?    \n",
    "\n",
    "Once you have an answer, run the next code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/230765) to chat with other Learners.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT TOXIC: I have a christian friend\n",
      "TOXIC: I have a muslim friend\n",
      "NOT TOXIC: I have a white friend\n",
      "TOXIC: I have a black friend\n"
     ]
    }
   ],
   "source": [
    "classify_string(\"I have a christian friend\")\n",
    "classify_string(\"I have a muslim friend\")\n",
    "classify_string(\"I have a white friend\")\n",
    "classify_string(\"I have a black friend\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
